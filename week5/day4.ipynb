{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0d1141",
   "metadata": {},
   "source": [
    "# RAG Day 4\n",
    "\n",
    "## Evaluation!\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Keep in mind how you would evaluate RAG for your business</h2>\n",
    "            <span style=\"color:#181;\">This is such an important part of building an accurate and reliable RAG pipeline. And it's applicable to many aspects of solving business problems with LLMs. People are often focused on RAG architecture and RAG frameworks for their business. But even more important: evaluations!</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60995166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbcfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = test.load_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4d88a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65fd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who won the prestigious IIOTY award in 2023?\n",
      "direct_fact\n",
      "Maxine Thompson won the prestigious Insurellm Innovator of the Year (IIOTY) award in 2023.\n",
      "['Maxine', 'Thompson', 'IIOTY']\n"
     ]
    }
   ],
   "source": [
    "example = tests[0]\n",
    "print(example.question)\n",
    "print(example.category)\n",
    "print(example.reference_answer)\n",
    "print(example.keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b41398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluation.test.TestQuestion"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f058ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'direct_fact': 70,\n",
       "         'temporal': 20,\n",
       "         'spanning': 20,\n",
       "         'comparative': 10,\n",
       "         'numerical': 10,\n",
       "         'relationship': 10,\n",
       "         'holistic': 10})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter([t.category for t in tests])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b413b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval import evaluate_retrieval, evaluate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daca435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalEval(mrr=0.6666666666666666, ndcg=0.6399069297160626, keywords_found=2, total_keywords=3, keyword_coverage=66.66666666666666)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925b37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval, answer, chunks = evaluate_answer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd34561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.feedback = \"The answer correctly identifies Maxine as the winner and specifies her role, aligning with the reference. However, it provides her full name and title but omits the full name 'Maxine Thompson,' which is a key detail from the reference. The answer is relevant and addresses the question directly but is somewhat less complete in matching the full detail of the reference.\"\n",
      "eval.accuracy = 4.0\n",
      "eval.completeness = 4.0\n",
      "eval.relevance = 5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{eval.feedback = }\")\n",
    "print(f\"{eval.accuracy = }\")\n",
    "print(f\"{eval.completeness = }\")\n",
    "print(f\"{eval.relevance = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e0cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
